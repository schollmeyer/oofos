#' Optimize a linear function on the family of all extents of a formal context
#'
#'
#' @description 'optimize_on_context_extents' maximizes a linear function over
#'  all indicator sets of all extents of a given formal context.
#'
#' @param context is the underlying formal context.
#' @param gen_index is an index vector that indicates the set of objects which
#' will be considered as the obects that generate the formal concept lattice
#' over which concept extents the optimization is done. By default it is the
#' vector of the indices of all objects of the context. One can show that the
#' maximization over the concept lattice that is generated only by all objects
#' with a positive objective value 'objective' will lead to the same
#' optimization problem as if one would optimize over the concept lattice
#' generated by all objects reference: TODO
#'
#' @param objective is a vector of length nrow(context) that represents the linear
#' function that is optimized.
#'
#' @param binary_variables one of the characters "afap", "sd", "allgen", "al"
#' (short for 'as few as possible', 'smallest dimension', 'all generating
#' objects' or 'all variables'. This determines which variables are set to
#' binary ones. Depending on this, the speed of the optimization can vary very
#' strongly.
#'
#' TODO genauer beschreiben
#' @return a model that can be optimized by calling gurobi(...).
#'
#' @examples
#' \dontrun{
#' # needs Package CDM (Cognitive Diagnosis Modelling)
#' dat <- na.omit(CDM::data.timss07.G4.lee$data)
#' context <- dat[,-(1:3)]
#' target <- compute_objective (dat,"girl",0)
#' model <- optimize_on_context_extents(context,(1:344),target)
#' result <- gurobi::gurobi(model)
#' result$objval}
#'
#'
#' context <- matrix(c(1,1,0,0,1,0,1,
#' 1,0,0,0,0,1,0,
#' 0,1,0,0,0,0,0,
#' 0,1,0,1,0,0,0,
#' 0,1,0,1,0,0,0,
#' 0,0,1,1,1,0,0,
#' 0,0,0,0,0,1,0,
#' 0,0,0,0,0,1,0,
#' 0,1,0,1,0,0,0,
#' 1,0,1,0,0,1,0), nrow = 10, ncol = 7, byrow = TRUE)
#'
#' objective <- c(1,2,3,4,5,6,7,8,9,10) - 5.5
#' model <- optimize_on_context_extents(context,(1:10),objective)
#' result <- gurobi::gurobi(model)
#'
#' result$x
#' # 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0
#'
#' result$objval
#' # [1] 5
#'
#'
#'
#' @export
optimize_on_context_extents <- function(context,
                                        gen_index = seq_len(nrow(context)),
                                        objective, binary_variables = "afap") {
  n_rows <- dim(context)[1]
  n_cols <- dim(context)[2]
  mask <- rep(0, n_rows)
  mask[gen_index] <- 1
  N <- 5 * (n_rows + n_cols)
  NN <- n_rows * n_cols - sum(context)
  I <- rep(as.integer(0), 5 * NN)
  J <- I
  V <- I

  lb <- rep(0, n_rows + n_cols)
  ub <- rep(1, n_rows + n_cols)

  rhs <- rep(0, N)
  sense <- rep("", N)
  t <- 1
  tt <- 1
  for (k in (1:n_rows)) {
    i <- which(context[k, ] == 0)

    if (length(i) >= 1) {
      L <- length(i)
      I[tt] <- t
      J[tt] <- k ## A[t,k]=L;
      V[tt] <- L
      tt <- tt + 1
      index <- (tt:(tt + L - 1))
      I[index] <- t
      J[index] <- i + n_rows ### A[t,i+m]=1;
      V[index] <- 1

      tt <- tt + L
      rhs[t] <- length(i)
      sense[t] <- "<="
      t <- t + 1
    } else {
      lb[k] <- 1
    }
  }


  for (k in (1:n_cols)) {
    i <- which(context[, k] == 0)

    if (length(i) >= 1) {
      L <- length(i)
      I[tt] <- t
      J[tt] <- k + n_rows # A[t,k+m]=L
      V[tt] <- L
      tt <- tt + 1
      index <- (tt:(tt + L - 1))
      I[index] <- t
      J[index] <- i #  A[t,i]=1;
      V[index] <- 1
      tt <- tt + L
      rhs[t] <- length(i)
      sense[t] <- "<="
      t <- t + 1
    } else {
      lb[k + n_rows] <- 1
    }
  }



  for (k in (1:n_rows)) {
    i <- which(context[k, ] == 0)
    L <- length(i)
    if (length(i) >= 1) {
      L <- length(i)
      I[tt] <- t
      J[tt] <- k # A[t,k]=1;
      V[tt] <- 1

      tt <- tt + 1

      index <- (tt:(tt + L - 1))
      I[index] <- t
      J[index] <- i + n_rows # A[t,i+m]=1;
      V[index] <- 1
      tt <- tt + L

      rhs[t] <- 1
      sense[t] <- ">="
      t <- t + 1
    }
  }




  for (k in (1:n_cols)) {
    j <- which(mask == 1 & context[, k] == 0)


    if (length(j) >= 1) {
      L <- length(j)

      I[tt] <- t
      J[tt] <- k + n_rows # A[t,k+n_rows]=1;
      V[tt] <- 1
      tt <- tt + 1
      index <- (tt:(tt + L - 1))
      I[index] <- t
      J[index] <- j # A[t,j]=1;
      V[index] <- 1
      tt <- tt + L
      sense[t] <- ">="
      rhs[t] <- 1
      t <- t + 1
    } else {
      lb[k + n_rows] <- 1
    }
  }

  tt <- tt - 1
  t <- t - 1

  jj <- which(I != 0 & J != 0)

  ###  setze je nach Methode gewisse Variablen als binaer
  vtypes <- rep("C", n_rows + n_cols)
  if (binary_variables == "afap") {
    if (length(gen_index) <= min(n_rows, n_cols)) {
      vtypes[gen_index] <- "B"
    }
    if (length(gen_index) > min(n_rows, n_cols) & n_rows <= n_cols) {
      vtypes[(1:n_rows)] <- "B"
    }
    if (length(gen_index) > min(n_rows, n_cols) & n_cols <= n_rows) {
      vtypes[-(1:n_rows)] <- "B"
    }
  }

  if (binary_variables == "sd") {
    if (n_rows <= n_cols) {
      vtypes[(1:n_rows)] <- "B"
    } else {
      vtypes[-(1:n_rows)] <- "B"
    }
  }

  if (binary_variables == "allgen") {
    vtypes[gen_index] <- "B"
    vtypes[-(1:n_rows)] <- "B"
  }

  if (binary_variables == "all") {
    vtypes <- rep("B", n_rows + n_cols)
  }

  if (!(binary_variables %in% c("afap", "sd", "allgen", "all"))) {
    print("invalid argument for binary_variabes")
  }
  return(list(
    A = slam::simple_triplet_matrix(I[jj], J[jj], V[jj],
      nrow = t,
      ncol = n_rows + n_cols
    ),
    rhs = rhs[(1:t)], sense = sense[(1:t)],
    modelsense = "max", lb = lb, ub = ub,
    obj = c(objective, rep(0, n_cols)),
    ext_obj = objective, intent_obj = rep(0, n_cols),
    n_rows = n_rows, n_cols = n_cols, vtypes = vtypes,
    n_constr = t, context = context
  ))
}

#' Compute the objective vector for linear optimization
#'
#' @description 'compute_objective computes the objective vector that
#' corresponds to the spremum type objective function
#'
#' max < w^x,m > - < w^y,m >
#'
#' where m ranges over all indicator functions of the considered
#' family of sets, cf, Schollmeyer et al. 2017, p.8f, p.20
#' @param dat is the underlying data set
#' @param target is a character that specifies which variable of the dataset dat
#'  serves as target.
#' @param target_class specifies for which value of the target variable the
#'  relative frequencies are considered.
#' @param weights is a possible weighing vector for the target variable
#' @return the vector of the objective function, namely
#' w^x-w^y where w^x are the relative frequencies of the target taking the value
#' 'target_class' (the denominator is the number of all data points with value
#' 'target_value'). More concretely w^x is 1/n^x if the coressponding data point
#' takes the value 'target_value' where n^x s the number of data points with
#' value 'target_value'. Otherwise w^x is zero. Analoguously, w^y = 1/n^y if the
#' corresponding data point has not the value 'target_value' where n^y is the
#' number of data points that have not the value 'target_value'. And w^y = 0
#' otherwise.
#'
#' @references Schollmeyer, G., Jansen, C., Augustin, T. (2017):
#' Detecting stochastic dominance for poset-valued random variables as an
#' example of linear programming on closure systems. Technical Report 209,
#' Department of Statistics, LMU Munich.
#'
#' @export
compute_objective <- function(dat, target, target_class, weights = rep(1, length(dat[[target]]))) {
  if (any(weights < 0)) {
    print("warning: negative weights")
  }
  i <- which(dat[[target]] == target_class)
  v <- rep(0, length(dat[[target]]))

  v[i] <- weights[i] / sum(weights[i])
  v[-i] <- -weights[-i] / sum(weights[-i])

  # v <- (dat[[target]]==target_class)-(dat[[target]]!=target_class)*mean(dat[[target]]==target_class)/mean(dat[[target]]!=target_class)
  return(v)
}



###
###
###
#  subgroup discovery

compute_quality <- function(sdtask, result, NAMES = colnames(sdtask$context)) { ## berechnet Piatetsky-Shapiro-Qualitätsfunktion für bereits geloestes Model (Variable result). Variable sdtask ist erzeugtes Modell aus Funktion subgroup.discovery.fca.milp
  m <- sdtask$n_rows

  idx <- which(result$x[seq_len(sdtask$n_rows)] > 0.5)
  jdx <- which(result$x[-(1:sdtask$n_rows)] > 0.5)
  n0 <- length(which(sdtask$obj[seq_len(sdtask$n_rows)] > 0))
  n <- length(idx)
  p <- length(which(sdtask$obj[seq_len(sdtask$n_rows)] > 0 & result$x[seq_len(sdtask$n_rows)] > 0.5)) / n
  p0 <- length(which(sdtask$obj > 0)) / sdtask$n_rows
  rho <- sqrt(n / m) * (p - p0) / sqrt((1 - n / m) * p0 * (1 - p0))
  return(list(n = n, n0 = n0, p = p, p0 = p0, ps = n * (p - p0), rho = rho, obj = result$objval, argmax = NAMES[jdx]))
}







optim_on_context_extents_h0 <- function(model, params =
                                               list(outputflag = 0)) {
  obj_new <- sample(model$obj[seq_len(nrow(model$context))])
  model$obj[seq_len(nrow(model$context))] <- obj_new
  result <- gurobi::gurobi(model, params = params)$objval
  return(result)
}

#' Compute a statistical significance test for an optimization over context
#' extents
#'
#' TODO : whole description
#'
#' @description 'compute_starshaped_distr_test' performs a statistical
#' significance test to assess if the largest differences of relative
#' frequencies over all starshaped subgroups that was found in a starshaped
#' subgroup discovery is statistically significantly diefferent from zero.
#'
#' @param ssd_result is the result of a starshaped subgroup discovery that was
#' computed with the function 'discover_starshaped_subgroups'.
#' @param n_rep is the number of permutations in the observation-randomization
#' test.
#'
#' @param plot_progress if TRUE (default) then the progress of the
#' observation-randomization test is plot by drawing the empirical distribution
#' function of the currently computed resampled objective values. Additionally,
#' nonparametric and parametric estimates of the p-value based on the currently
#' computed resamples are printed. TODO : beta genauer
#'
#' @return A list with entries 'objvalues': the resampled objective values,
#' 'p_value'  the (nonparametrically) estimated p-value of the
#' observation-randomization test, 'p_value_parametric': a parametric estimate
#' of the p-value based on a beta-approximation of the distribution of the
#' resampled objective values.
#'
#'
#' @export
compute_extent_optim_test <- function(model, n_rep = 1000,
                                          plot_progress = TRUE) {
  if (n_rep <= 2) {
    print("Be serious!")
    return(NULL)
  }
  objvalues <- rep(0, n_rep)
  for (k in seq_len(n_rep)) {
    objvalues[k] <- optim_on_context_extents_h0(model)
    x <- objvalues[seq_len(k)]
    p_value <- mean(x >= model$objval)
    if (k > 2) {
      suppressWarnings(fit <- fit_ks_distribution(x))
      p_value_parametric <- pnorm((model$objval-mean(x))/sd(x),lower.tail=FALSE)
      # old version
      #p_value_parametric <- stats::pbeta(model$objval, fit$par[1],
      #                                   fit$par[2], fit$par[3],
      #                                   lower.tail = FALSE
      #)
    }

    if (plot_progress == TRUE & k > 2) {
      plot(stats::ecdf(x),
           do.points = FALSE, col.01line = NULL,
           main = paste(
             "observed value:",
             round(model$objval, 4),
             "p-palue:", round(p_value, 4), "\n; param. p-value:",
             round(p_value_parametric, 4), "; n:", k, "\nmedian:", round(stats::median(x),4)
           ), verticals = TRUE,
           xlab = "test statistic", ylab = "cdf (black), density (grey)",
           xlim = c(0.95*min(x), 1.05 * max(c(x, model$objval)))
      )
      graphics::abline(v = model$objval, col = "darkblue")
      graphics::abline(v = stats::median(x), col = "darkgreen", lty = 2)
      sort_x <- seq(0, 1, length.out = 1000)
      density_parametric <- stats::dbeta(
        sort_x, fit$par[1], fit$par[2],
        fit$par[3]
      )
      graphics::lines(sort_x, density_parametric / max(density_parametric),
                      col = "darkgreen"
      )
      cdf_parametric <- stats::pbeta(sort_x, fit$par[1], fit$par[2], fit$par[3])
      graphics::lines(sort_x, cdf_parametric, col = "darkgreen")
      f <- stats::density(x)
      graphics::lines(f$x, f$y / max(f$y), col = "grey")
      # TODO :result_temp <<- list(objvalues = x, p_value = p_value,
      #                           p_value_parametric =
      #   p_value_parametric)
    }
  }
  #rm(result_temp)
  return(list(
    objvalues = objvalues, p_value = p_value, p_value_parametric =
      p_value_parametric
  ))
}



quality <- function(model,result,NAMES=colnames(model$context)){


  ## berechnet Piatetsky-Shapiro-Qualitätsfunktion für bereits geloestes Model (Variable result). Variable sdtask ist erzeugtes #Modell aus Funktion subgroup.discovery.fca.milp
  ## computes quality function for a subgroup
  # TODO description etc

  idx <- which(result$x[seq_len(model$n_rows)]>0.5)
  jdx <- which(result$x[-seq_len(model$n_rows)]>0.5)
  n0 <- length(which(model$obj[seq_len(model$n_rows)]>0))
  n <- length(idx)
  p <- length(which(model$obj[seq_len(model$n_rows)]>0 & result$x[seq_len(model$n_rows)]>0.5))/n
  p0 <- length(which(model$obj>0))/model$n_rows
  return(list(n=n,n0=n0,p=p,p0=p0,piatetsky_shapiro=n*(p-p0),wracc=n*(p-p0)*model$n_rows,lift=p/p0,kolmogorov_smirnov=result$objval,obj=result$objval,argmax=NAMES[jdx],
              argmax_extreme_points=NAMES[



                get_extreme_attributes(result$x[-seq_len(model$n_rows)],model$context )]))

}

# TODO description etc.

calculate_nominal_scaling_vec <- function(data_values, add_column_name=NULL) {
  # nominal scaling the vector

  # Input: data values(vector): for each observation one factor value
  #         add_column_name (NULL, char): a further definition for the column names

  # Output: dataframe representing the crosstable

  attr <- sort(unique(data_values))
  length_attr <- length(attr)
  number_elements_data <- length(data_values)

  # Memory sapce
  context_logical <- array(0, c(number_elements_data, length_attr))
  column_names_context <- rep("", length_attr)

  # Looping throw all attributes
  for (k in 1:length_attr) {
    # Defining the column name
    column_names_context[k] <- paste(c(add_column_name, ": ", as.character(attr[k])), collapse = "")
    # Defining the entries of the column. TRUE if value has attribute k
    context_logical[ ,k] <- as.integer(data_values == attr[k])

  }
  colnames(context_logical) <- column_names_context
  return(context_logical)
}

calculate_ordinal_scaling_vec <- function(data_values, add_column_name = NULL) {
  # ordinal scaling the vector

  # Input: data values (vector): for each observation the corresponding numeric value
  #         add_column_name (NULL, char): a further definition for the column names

  # Output: dataframe representing the crosstable

  data_values <- as.numeric(as.character(data_values))

  attr <- sort(unique(data_values))
  length_attr <- length(attr)
  number_elements_data <- length(data_values)

  # Memory space
  context_logical <- array(0, c(number_elements_data,length_attr))
  colnames_context <- rep("",length_attr)

  # Loop throw all attributes
  t = 1
  for (k in (1:length_attr)) {
    # Defining the column name
    colnames_context[k] <- paste(c(add_column_name,": x<=", attr[k]), collapse = "")
    # Defining the entries of the column. TRUE if value is smalles than attr
    context_logical[ ,k] <- (data_values <= attr[k])
  }

  # Changing the colnames of the produced context
  colnames(context_logical) <- colnames_context

  return(context_logical)
}


calculate_dual_ordinal_scaling_vec <- function(data_values, add_column_name = NULL) {
  # dual-ordinal (meaning >= instead of <=) scaling the vector

  # Input: data values (vector): for each observation the corresponding numeric value
  #         add_column_name (NULL, char): a further definition for the column names

  # Output: dataframe representing the crosstable

  data_values <- as.numeric(as.character(data_values))

  attr <- sort(unique(data_values))
  lenght_attr <- length(attr)
  number_elements_data <- length(data_values)

  # Memory space
  context_logical <- array(0,c(number_elements_data,lenght_attr))
  colnames_context <- rep("",lenght_attr)

  # Loop throw all attributes
  t = 1
  for (k in (1:lenght_attr)) {
    # Defining the column name
    colnames_context[k] <- paste(c(add_column_name, ": x>=", attr[k]), collapse = "")
    # Defining the entries of the column. TRUE if value is larger or equal attr
    context_logical[ ,k] <- (data_values >= attr[k])
  }

  # Changing the colnames of the produced context
  colnames(context_logical) <- colnames_context

  return(context_logical)
}



# Auxiliary function for calculate_conceptual_scaling
calculate_number_columns_attr <- function(data_matrix){
  # This function is needed to calculate the number of attributes needed to
  # represent each column. This depends on the used class of the values in the
  # column.

  # Input: data_matrix (dataframe): each row represents one attribute
  #                                 (not necessarily two-valued)

  # Output (list): number of attributes needed (all and per attribute),
  #                 the column names and the class of the elements

  number_attr <- dim(data_matrix)[2]
  colnames_data <- colnames(data_matrix)

  # Memory space
  number_column_per_attr <- rep(0, number_attr)
  column_names <- NULL
  class_elements_columns <- rep("", number_attr)

  # Looping throw all attrbutes given by the input
  for (k in (1:number_attr)) {
    # Saving which mode have the elements in column k
    class_elements_columns[k] <- class(data_matrix[ ,k])

    # if the mode is "ordered", "numeric" or "integer" the values are ordinal and
    # dual-ordinal saved and hence we have 2*(number of different values in column k)
    if (class(data_matrix[,k])[1] == "ordered" |
        class(data_matrix[,k])[1] == "numeric" |
        class(data_matrix[,k])[1] == "integer") {
      number_column_per_attr[k] <- 2 * length(unique(data_matrix[ ,k]))
    }
    # if the mode of column k is "factor" for each value one column is needed
    if (class(data_matrix[,k])[1] == "factor" ) {
      number_column_per_attr[k] <- length(unique(data_matrix[,k]))
    }

    # Now we save the colnames by adding to the already defined ones
    column_names <- c(column_names, rep(colnames_data[k], number_column_per_attr[k]))
  }

  return(list(class_elements_columns = class_elements_columns,
              number_column_per_attr = number_column_per_attr,
              number_column_all_attr = sum(number_column_per_attr),
              column_names = column_names))
}


get_auto_conceptual_scaling <- function(data_matrix,print_scalings=TRUE) {
  # main function for scaling automatically

  # Input: data_matrix (dataframe): each row represents one attribute
  #                                 (not necessarily two-valued)

  # Output (matrix): Scaled crosstable

  number_obj <- dim(data_matrix)[1]
  number_attr <- dim(data_matrix)[2]
  colnames_data <- colnames(data_matrix)

  number_columns_needed <- calculate_number_columns_attr(data_matrix)

  # Memory spaces
  context_converted <- array(0,c(number_obj, number_columns_needed$number_column_all_attr))
  colname_context <- rep("", number_columns_needed$number_column_all_attr)

  t = 1
  for (k in (1:number_attr)) {
    if(print_scalings==TRUE) { print(c(k,": ",class(data_matrix[,k])),quote=FALSE)}

    if (class(data_matrix[ ,k])[1] == "ordered" |
        class(data_matrix[ ,k])[1] == "numeric" |
        class(data_matrix[ ,k])[1] == "integer") {
      # Calculating the context for the attribute k
      inner_context  <-  cbind(calculate_ordinal_scaling_vec(as.numeric(data_matrix[ ,k]),
                                                             colnames_data[k]),
                               calculate_dual_ordinal_scaling_vec(as.numeric(data_matrix[ ,k]),
                                                                  colnames_data[k]))
      # number of columns needed for saving
      column_number_k <- number_columns_needed$number_column_per_attr[k] - 1

      # Saving
      context_converted[ , t:(t + column_number_k)] <- inner_context
      colname_context[t:(t + column_number_k)] <- colnames(inner_context)

      t <- t + column_number_k + 1
    }
    if (class(data_matrix[,k])[1] == "factor") {
      # Calculating the context for the attribute k
      inner_context <- calculate_nominal_scaling_vec(data_matrix[,k],
                                                     colnames_data[k])
      # number of columns needed for saving
      column_number_k <- number_columns_needed$number_column_per_attr[k] - 1

      # Saving
      context_converted[ , t:(t + column_number_k)] <- inner_context
      colname_context[t:(t + column_number_k)] <- colnames(inner_context)

      t <- t + column_number_k + 1
    }
  }

  # Changing colnames
  colnames(context_converted) <- colname_context

  return(context_converted)
}

min_k_attr_generated=function(extent,intent,X){  # Berecchnet für Begriff gegeben durch Umfang extent und Inhalt intent das maximale k, für das der Begriff k-Merkmalserzeugt ist (Kontext X muss ebenfalls mit übergeben werden)
  m=dim(X)[1]
  n=dim(X)[2]
  model=k_extent_opt_b(X,(1:m),(1:m),K=dim(X)[2])
  model$lb[which(extent==1)]=1
  model$ub[which(extent==0)]=0
  model$ub[which(intent==0)+m]=0
  model$obj=c(rep(0,m),rep(1,n))
  model$modelsense="min"
  return(model)}


min_k_obj_generated=function(extent,intent,X){min_k_attr_generated(intent,extent,t(X))} # Berecchnet für Begriff gegeben durch Umfang extent und Inhalt intent das maximale k, für das der Begriff k-MGegenstandserzeugt ist (Kontext X muss ebenfalls mit übergeben werden)

k_extent_opt_b=function(X,gen_index,v,binary_variables="afap",K){##  extentopt: Version, wie TR 209, S.23 beschrieben, nur Ungleichungen (21) verschärft
  # adaptiert, so dass Modell zur Optimierung von Zielfunktion v über alle K-Merkmalserzeugte Begriffe berchnet wird
  m=dim(X)[1]
  n=dim(X)[2]
  mask=rep(0,m)
  mask[gen_index]=1
  N=2*(m+n)

  A= array(0,c(N,m+n))
  rhs=rep(0,N)
  sense=rep("",N)
  t=1
  for(k in (1:m)){
    i=which(X[k,]==0)
    if(length(i)>=1){
      A[t,k]=length(i);A[t,i+m]=1;rhs[t]=length(i);sense[t]="<="
      t=t+1
    }

  }

  for(k in (1:n)){
    i=which(X[,k]==0)
    if(length(i)>=1){
      A[t,k+m]=length(i);A[t,i]=1;rhs[t]=length(i);sense[t]="<="
      t=t+1
    }

  }

  for(k in (1:m)){
    i=which(X[k,]==0)
    if(length(i)>=1){
      A[t,which(X[k,]==0)+m]=1;A[t,k]=1;rhs[t]=1;sense[t]=">=";t=t+1
    }}

  # for(k in (1:n)){
  #j=which(mask==1 & X[,k]==0)
  #if(length(j)>=1){
  #A[t,which(mask==1 & X[,k]==0)]=1;A[t,k+m]=1;sense[t]=">=";rhs[t]=1;t=t+1}}
  t=t-1





  lb=rep(0,m+n)
  ub= rep(1,m+n)
  idx=which(colSums(X[gen_index,])==length(gen_index))
  if(length(idx)>0){lb[m+idx]=1}

  idx=which(rowSums(X[,])==n)
  if(length(idx)>0){lb[idx]=1}

  ###  setze je nach Methode gewisse Variablen als binaer
  vtypes=rep("C",m+n)
  if(binary_variables=="afap"){
    if(length(gen_index)<=min(m,n)){
      vtypes[gen_index]="B"
    }
    if(length(gen_index)>min(m,n) & m<=n){
      vtypes[(1:m)]="B"
    }
    if(length(gen_index)>min(m,n) & n<=m){
      vtypes[-(1:m)]="B"
    }
  }

  if(binary_variables=="sd"){
    if(m<=n){
      vtypes[(1:m)]="B"
    }
    else{vtypes[-(1:m)]="B"}
  }

  if(binary_variables=="allgen"){
    vtypes[gen.index]="B"
    vtypes[-(1:m)]="B"
  }

  if(binary_variables=="all"){
    vtypes=rep("B",m+n)
  }

  if(  !(binary_variables %in% c("afap","sd","allgen","all"))){print("invalid argument for binary.variabes")}

  A=rbind(A[(1:t),],c(rep(0,m),rep(1,n)))
  rhs=c(rhs[(1:t)],K)
  sense=c(sense[(1:t)],"<")
  ##Über vtypes nochmalnachdenken:

  vtypes[(1:m)]="C"
  vtypes[-(1:m)]="B"

  return(list(A=as.simple_triplet_matrix(A),rhs=rhs,sense=sense,modelsense="max",lb=lb,ub=ub,obj=c(v,rep(0,n)),ext.obj=v,intent.obj=rep(0,n),m=m,n=n,vtypes=vtypes,n.constr=t,context=X))}






#
#
# subgroup_discovery_fca_milp <- function(dat, target, target.class, nrep, heuristic, remove.full.columns = TRUE, clarify.cols = FALSE, reduce.cols = FALSE, weighted = FALSE, small) { # Fuehrt Subgroup Discovery durch (erzeugt nur MILP, das dann noch mit z.B. gurobi(...) gelöst werden muss
#   XX <- dat
#   XX[[target]] <- NULL
#   print(objects(XX))
#   X <- conceptual.scaling(XX)
#   print("E")
#
#
#
#   print(c("dim dat: ", dim(dat)), quote = FALSE)
#   print(c("dim X (conceptual scaling dat):     ", dim(X)), quote = FALSE)
#
#   if (remove.full.columns) {
#     X <- remove.full.cols(X)
#     print(c("dim X without full columns:         ", dim(X)), quote = FALSE)
#   }
#   if (clarify.cols) {
#     X <- col.clarify(X)
#     print(c("dim X without doubled columns:      ", dim(X)), quote = FALSE)
#   }
#   if (reduce.cols) {
#     X <- col.reduce(X)
#     print(c("dim final X (clarified and reduced):", dim(X)), quote = FALSE)
#   }
#
#
#   m <- dim(X)[1]
#   n <- dim(X)[2]
#
#   M <- dim(XX)[2]
#
#   # A=NULL
#   # A=array(0,c(2*n,m+n))
#   # t=1
#   # T=1
#   # for(k in (1:M)){
#   # K=length(unique(XX[,k]))
#   # #print(K)
#   # temp=rep(0,c(m+n))
#   # if(class(XX[,k])[1]=="factor"){
#
#   # temp[((t+m):(t+m+K-1))]=1
#   # A[t,]=temp
#   # T=T+1
#   # #A=rbind(A,temp)
#
#   # t=t+K
#   # }
#
#   # if(class(XX[,k])[1]=="ordered" | class(XX[,k])[1]=="numeric" | class(XX[,k])[1]=="integer"){
#
#   # for(l in (1:(K-1))){
#   # temp[t+m+l-1]=1
#   # temp[t+m+l]=-1
#   # A[T,]=temp
#   # T=T+1
#   # #A=rbind(A,temp)
#
#   # temp=rep(0,c(m+n))
#   # temp[t+m+l-1+K]=-1
#   # temp[t+m+l+K]=1
#   # A[T,]=temp
#   # T=T+1
#   # #=rbind(A,temp)
#   # }
#   # t=t+2*K
#   # }
#
#
#
#   # #t=t+K
#   # }
#   # T=T-1
#   # A=A[(1:T),]
#   # A=NULL
#   v <- (dat[[target]] == target.class) - (dat[[target]] != target.class) * mean(dat[[target]] == target.class) / mean(dat[[target]] != target.class)
#   if (weighted) {
#     W <- weighted.repr(X, v)
#     v <- W$yw
#
#     X <- W$Xw
#   } else {
#     W <- list(count = rep(1, dim(X)[1]))
#   }
#
#   v <<- v
#   m <- dim(X)[1]
#   n <- dim(X)[2]
#   print("building model...")
#   if (small) {
#     ans <- extent.opt(X, which(v > 0), v)
#   } else {
#     ans <- extent.opt(X, (1:m), v)
#   }
#   print("...done")
#   # M=dim(A)[1]
#   # ans$A=rbind(ans$A,A);ans$rhs=c(ans$rhs,rep(1,M));ans$sense=c(ans$sense,rep("<=",M))
#   print(system.time(temp <- heuristic(X, v, nrep = nrep)))
#   CUT <- ceiling(temp$objval)
#   print(CUT)
#
#   T <- rep(0, n + m)
#   T[(which(v > 0))] <- -W$count[which(v > 0)]
#   # ans$A=rbind(ans$A,matrix(T,nrow=1))
#   # ans$rhs=c(ans$rhs,-CUT)
#   # ans$sense=c(ans$sense,"<=")
#   ans$start <- c(temp$solution, PSI(temp$solution, context = X))
#   ans$context <- X
#   ans$NAMES <- conceptual.scaling.dim(XX)$NAMES
#   # TEMP=heuristic.implications(X,v,NREP)
#
#   # ans$A=rbind(ans$A,TEMP$A);ans$rhs=c(ans$rhs,TEMP$rhs);ans$sense=c(ans$sense,TEMP$sense)
#
#   # ans$vtypes[which(ans$start>=0.5)]="B"
#   # ans$vtypes=rep("B",length(ans$vtypes))
#   return(ans)
# }
